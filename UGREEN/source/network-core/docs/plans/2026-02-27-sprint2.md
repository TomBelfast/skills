# Sprint 2 — Network Core Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Rozszerzyć backend o brakujące endpoint, prawdziwy health-check DB, zamontować dane Nmap do Dockera, zbudować scaffold frontendu z mapą topologii (React + Cytoscape) oraz zaimplementować worker do monitoringu Ping.

**Architecture:** Backend FastAPI + SQLAlchemy async, frontend Vite+React+Cytoscape.js, worker Python async. Wszystko uruchamiane przez Docker Compose. Testy integracyjne używają SQLite in-memory (jak w Sprint 1).

**Tech Stack:** FastAPI, SQLAlchemy async, Pydantic v2, pytest+httpx+aiosqlite, Vite, React 18, Cytoscape.js, asyncio, icmplib (lub subprocess ping)

**Working directory:** `C:\APLIKACJE\UGREEN\source\network-core`

---

## Task 1: GET /api/v1/devices/{id}

**Files:**
- Modify: `backend/app/api/v1/devices.py`
- Modify: `backend/app/services/device_service.py`
- Test: `tests/integration/test_devices_api.py`

**Step 1: Napisz failing test**

Dodaj do `tests/integration/test_devices_api.py`:

```python
@pytest.mark.asyncio
async def test_get_device_by_id(client):
    create = await client.post("/api/v1/devices", json={"ip_address": "10.9.0.1"})
    device_id = create.json()["id"]
    resp = await client.get(f"/api/v1/devices/{device_id}")
    assert resp.status_code == 200
    assert resp.json()["id"] == device_id
    assert resp.json()["ip_address"] == "10.9.0.1"


@pytest.mark.asyncio
async def test_get_device_by_id_not_found(client):
    resp = await client.get("/api/v1/devices/nonexistent-id")
    assert resp.status_code == 404
```

**Step 2: Uruchom test — sprawdź że FAIL**

```bash
cd backend
pytest ../tests/integration/test_devices_api.py::test_get_device_by_id -v
```
Oczekiwane: FAIL — 404 lub 405 (endpoint nie istnieje)

**Step 3: Dodaj metodę get_device do serwisu**

W `backend/app/services/device_service.py` dodaj:

```python
async def get_device(db: AsyncSession, device_id: str) -> Device | None:
    result = await db.execute(select(Device).where(Device.id == device_id))
    return result.scalars().first()
```

**Step 4: Dodaj endpoint GET /api/v1/devices/{id}**

W `backend/app/api/v1/devices.py` dodaj po istniejących routach:

```python
@router.get("/devices/{device_id}", response_model=DeviceRead)
async def get_device(device_id: str, db: AsyncSession = Depends(get_db)):
    device = await device_service.get_device(db, device_id)
    if not device:
        raise HTTPException(status_code=404, detail="Device not found")
    return device
```

**Step 5: Uruchom wszystkie testy — sprawdź że PASS**

```bash
cd backend
pytest ../tests/ -v
```
Oczekiwane: 24 passed

**Step 6: Commit**

```bash
git add backend/app/api/v1/devices.py backend/app/services/device_service.py tests/integration/test_devices_api.py
git commit -m "feat: add GET /api/v1/devices/{id} endpoint with 404 handling"
```

---

## Task 2: health/ready z prawdziwym sprawdzeniem DB

**Files:**
- Modify: `backend/app/main.py`
- Test: `tests/integration/test_health.py`

**Step 1: Napisz failing test**

Zamień zawartość `tests/integration/test_health.py` na:

```python
import pytest


@pytest.mark.asyncio
async def test_health_live(client):
    resp = await client.get("/health/live")
    assert resp.status_code == 200
    assert resp.json()["status"] == "ok"


@pytest.mark.asyncio
async def test_health_ready(client):
    resp = await client.get("/health/ready")
    assert resp.status_code == 200
    data = resp.json()
    assert data["status"] == "ok"
    assert "db" in data


@pytest.mark.asyncio
async def test_health_ready_has_db_status(client):
    resp = await client.get("/health/ready")
    assert resp.status_code == 200
    assert resp.json()["db"] == "ok"
```

**Step 2: Uruchom test — sprawdź że FAIL**

```bash
cd backend
pytest ../tests/integration/test_health.py::test_health_ready_has_db_status -v
```
Oczekiwane: FAIL — `"db"` nie ma w odpowiedzi

**Step 3: Zmodyfikuj health/ready w main.py**

Zamień istniejący endpoint `health_ready` w `backend/app/main.py`:

```python
from sqlalchemy import text
from app.db.session import get_db

@app.get("/health/ready")
async def health_ready(db: AsyncSession = Depends(get_db)):
    try:
        await db.execute(text("SELECT 1"))
        db_status = "ok"
    except Exception:
        db_status = "error"
    return {"status": "ok", "db": db_status}
```

Dodaj na górze pliku potrzebne importy:
```python
from sqlalchemy.ext.asyncio import AsyncSession
from fastapi import Depends
from sqlalchemy import text
from app.db.session import get_db
```

**Step 4: Uruchom wszystkie testy — sprawdź że PASS**

```bash
cd backend
pytest ../tests/ -v
```
Oczekiwane: 25 passed (lub więcej z Task 1)

**Step 5: Commit**

```bash
git add backend/app/main.py tests/integration/test_health.py
git commit -m "feat: health/ready now performs actual DB connectivity check"
```

---

## Task 3: Docker volume dla plików Nmap seed

**Files:**
- Modify: `deploy/docker-compose.yml`

**Step 1: Zaktualizuj docker-compose.yml**

Zamień plik `deploy/docker-compose.yml` na:

```yaml
version: "3.9"

services:
  postgres:
    image: postgres:16-alpine
    environment:
      POSTGRES_USER: netcore
      POSTGRES_PASSWORD: netcore
      POSTGRES_DB: netcore
    ports:
      - "5432:5432"
    volumes:
      - pg_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U netcore"]
      interval: 5s
      timeout: 5s
      retries: 5

  api:
    build:
      context: ../backend
      dockerfile: Dockerfile
    environment:
      DATABASE_URL: postgresql+asyncpg://netcore:netcore@postgres:5432/netcore
      DEBUG: "false"
    ports:
      - "8000:8000"
    volumes:
      - ../../network-diagnostics-output:/nmap-data:ro
    depends_on:
      postgres:
        condition: service_healthy

volumes:
  pg_data:
```

**Step 2: Zaktualizuj NMAP_DIR w nmap_service.py**

W `backend/app/services/nmap_service.py` zmień linię NMAP_DIR aby obsługiwała oba środowiska:

```python
import os
from pathlib import Path

# Docker: /nmap-data, dev: ../../../../network-diagnostics-output
_docker_path = Path("/nmap-data")
_dev_path = Path(__file__).parents[5] / "network-diagnostics-output"
NMAP_DIR = _docker_path if _docker_path.exists() else _dev_path
```

**Step 3: Uruchom testy — sprawdź że PASS (nie powinno się nic zepsuć)**

```bash
cd backend
pytest ../tests/ -v
```
Oczekiwane: wszystkie passing

**Step 4: Commit**

```bash
git add deploy/docker-compose.yml backend/app/services/nmap_service.py
git commit -m "feat: mount nmap-data volume in Docker, auto-detect path in nmap_service"
```

---

## Task 4: Frontend React + Cytoscape — scaffold i mapa topologii

**Files:**
- Create: `frontend/` (cały scaffold)
- Create: `frontend/src/App.jsx`
- Create: `frontend/src/components/TopologyMap.jsx`
- Create: `frontend/src/api/devices.js`
- Modify: `deploy/docker-compose.yml` (dodaj service frontend)

**Step 1: Zainicjuj projekt Vite+React w frontend/**

```bash
cd source/network-core
# Usuń placeholder
rm frontend/.gitkeep

# Stwórz projekt Vite (NIE interaktywnie — użyj flag)
npm create vite@latest frontend -- --template react
cd frontend
npm install
npm install cytoscape axios
```

**Step 2: Stwórz plik `frontend/src/api/devices.js`**

```javascript
import axios from 'axios';

const API_BASE = import.meta.env.VITE_API_URL || 'http://localhost:8000';

export async function fetchDevices() {
  const { data } = await axios.get(`${API_BASE}/api/v1/devices`);
  return data;
}

export async function fetchLinks() {
  const { data } = await axios.get(`${API_BASE}/api/v1/links`);
  return data;
}
```

**Step 3: Stwórz `frontend/src/components/TopologyMap.jsx`**

```jsx
import { useEffect, useRef } from 'react';
import cytoscape from 'cytoscape';

export default function TopologyMap({ devices, links }) {
  const containerRef = useRef(null);
  const cyRef = useRef(null);

  useEffect(() => {
    if (!containerRef.current) return;

    const nodes = devices.map(d => ({
      data: {
        id: d.id,
        label: d.label || d.hostname || d.ip_address,
        type: d.device_type || 'unknown',
      },
    }));

    const edges = links.map(l => ({
      data: {
        id: l.id,
        source: l.source_id,
        target: l.target_id,
        label: l.link_type,
      },
    }));

    cyRef.current = cytoscape({
      container: containerRef.current,
      elements: [...nodes, ...edges],
      style: [
        {
          selector: 'node',
          style: {
            label: 'data(label)',
            'background-color': '#4f90d4',
            color: '#fff',
            'text-valign': 'bottom',
            'font-size': '11px',
          },
        },
        {
          selector: 'node[type="router"]',
          style: { 'background-color': '#e67e22' },
        },
        {
          selector: 'node[type="server"]',
          style: { 'background-color': '#27ae60' },
        },
        {
          selector: 'node[type="nas"]',
          style: { 'background-color': '#8e44ad' },
        },
        {
          selector: 'edge',
          style: {
            'line-color': '#aaa',
            width: 2,
            'curve-style': 'bezier',
          },
        },
      ],
      layout: { name: 'cose', animate: false },
    });

    return () => cyRef.current?.destroy();
  }, [devices, links]);

  return <div ref={containerRef} style={{ width: '100%', height: '600px', background: '#1a1a2e' }} />;
}
```

**Step 4: Zastąp `frontend/src/App.jsx`**

```jsx
import { useState, useEffect } from 'react';
import TopologyMap from './components/TopologyMap';
import { fetchDevices, fetchLinks } from './api/devices';

export default function App() {
  const [devices, setDevices] = useState([]);
  const [links, setLinks] = useState([]);
  const [error, setError] = useState(null);

  useEffect(() => {
    Promise.all([fetchDevices(), fetchLinks()])
      .then(([devs, lnks]) => {
        setDevices(devs);
        setLinks(lnks);
      })
      .catch(err => setError(err.message));
  }, []);

  return (
    <div style={{ fontFamily: 'sans-serif', padding: '1rem', background: '#0d0d1a', minHeight: '100vh', color: '#eee' }}>
      <h1 style={{ margin: '0 0 1rem' }}>Network Topology</h1>
      {error && <p style={{ color: 'red' }}>Error: {error}</p>}
      <p style={{ color: '#aaa', marginBottom: '0.5rem' }}>
        Devices: {devices.length} | Links: {links.length}
      </p>
      <TopologyMap devices={devices} links={links} />
    </div>
  );
}
```

**Step 5: Stwórz `frontend/.env.example`**

```
VITE_API_URL=http://localhost:8000
```

**Step 6: Dodaj frontend do docker-compose.yml**

W `deploy/docker-compose.yml` dodaj service `frontend` po `api`:

```yaml
  frontend:
    image: node:20-alpine
    working_dir: /app
    command: sh -c "npm install && npm run dev -- --host 0.0.0.0 --port 5173"
    volumes:
      - ../frontend:/app
    ports:
      - "5173:5173"
    environment:
      VITE_API_URL: http://localhost:8000
    depends_on:
      - api
```

**Step 7: Zweryfikuj że frontend się uruchamia lokalnie**

```bash
cd source/network-core/frontend
npm run dev
```
Oczekiwane: Vite server na http://localhost:5173 bez błędów kompilacji

**Step 8: Commit**

```bash
cd source/network-core
git add frontend/ deploy/docker-compose.yml
git commit -m "feat: scaffold frontend React+Cytoscape with topology map, add to docker-compose"
```

---

## Task 5: Worker — monitoring Ping

**Files:**
- Create: `worker/requirements.txt`
- Create: `worker/ping_worker.py`
- Create: `worker/Dockerfile`
- Modify: `deploy/docker-compose.yml` (dodaj service worker)
- Test: `tests/unit/test_ping_worker.py`

**Step 1: Stwórz `worker/requirements.txt`**

```
asyncpg==0.30.0
sqlalchemy[asyncio]==2.0.36
```

**Step 2: Napisz failing test w `tests/unit/test_ping_worker.py`**

```python
import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../../worker'))

import pytest
from unittest.mock import patch, AsyncMock
from ping_worker import ping_host, classify_result


def test_classify_result_alive():
    assert classify_result(True) == "alive"


def test_classify_result_unreachable():
    assert classify_result(False) == "unreachable"


@pytest.mark.asyncio
async def test_ping_host_success():
    with patch("ping_worker.asyncio.create_subprocess_exec", new_callable=AsyncMock) as mock_proc:
        mock_proc.return_value.wait = AsyncMock(return_value=0)
        result = await ping_host("127.0.0.1")
    assert result is True


@pytest.mark.asyncio
async def test_ping_host_failure():
    with patch("ping_worker.asyncio.create_subprocess_exec", new_callable=AsyncMock) as mock_proc:
        mock_proc.return_value.wait = AsyncMock(return_value=1)
        result = await ping_host("192.0.2.1")
    assert result is False
```

**Step 3: Uruchom test — sprawdź że FAIL**

```bash
cd backend
pytest ../tests/unit/test_ping_worker.py -v
```
Oczekiwane: FAIL — `ping_worker` nie istnieje

**Step 4: Stwórz `worker/ping_worker.py`**

```python
"""
Ping Worker — sprawdza dostępność urządzeń przez ICMP ping.

Uruchamia się jako pętla: co INTERVAL sekund pinguje wszystkie urządzenia
i aktualizuje pole `status` w tabeli devices.

Zmienne środowiskowe:
  DATABASE_URL  — postgresql+asyncpg://user:pass@host/db
  INTERVAL      — czas między rundami (domyślnie 60 sekund)
"""
import asyncio
import os
import sys
import logging

from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker
from sqlalchemy import text

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
log = logging.getLogger("ping_worker")

DATABASE_URL = os.environ.get("DATABASE_URL", "")
INTERVAL = int(os.environ.get("INTERVAL", "60"))


def classify_result(alive: bool) -> str:
    return "alive" if alive else "unreachable"


async def ping_host(ip: str) -> bool:
    """Pinguje hosta — zwraca True jeśli odpowiada."""
    try:
        proc = await asyncio.create_subprocess_exec(
            "ping", "-c", "1", "-W", "2", ip,
            stdout=asyncio.subprocess.DEVNULL,
            stderr=asyncio.subprocess.DEVNULL,
        )
        rc = await proc.wait()
        return rc == 0
    except Exception:
        return False


async def run_round(session: AsyncSession) -> None:
    """Jedna runda: pobierz urządzenia, pinguj, zapisz status."""
    result = await session.execute(text("SELECT id, ip_address FROM devices"))
    devices = result.fetchall()
    if not devices:
        log.info("No devices found.")
        return

    tasks = [(row[0], row[1]) for row in devices]
    results = await asyncio.gather(*[ping_host(ip) for _, ip in tasks])

    for (device_id, ip), alive in zip(tasks, results):
        status = classify_result(alive)
        await session.execute(
            text("UPDATE devices SET status = :status WHERE id = :id"),
            {"status": status, "id": device_id},
        )
        log.info("%-16s  %s", ip, status)

    await session.commit()


async def main() -> None:
    if not DATABASE_URL:
        log.error("DATABASE_URL is not set. Exiting.")
        sys.exit(1)

    engine = create_async_engine(DATABASE_URL, echo=False)
    session_factory = async_sessionmaker(engine, expire_on_commit=False)

    log.info("Ping worker started. Interval: %ds", INTERVAL)
    while True:
        async with session_factory() as session:
            try:
                await run_round(session)
            except Exception as exc:
                log.error("Round failed: %s", exc)
        await asyncio.sleep(INTERVAL)


if __name__ == "__main__":
    asyncio.run(main())
```

**Step 5: Uruchom testy — sprawdź że PASS**

```bash
cd backend
pytest ../tests/unit/test_ping_worker.py -v
```
Oczekiwane: 4 passed

**Step 6: Dodaj kolumnę `status` do modelu Device**

W `backend/app/models/device.py` dodaj pole:
```python
status: Mapped[str] = mapped_column(String(32), nullable=False, server_default="unknown")
```

**Step 7: Stwórz migrację Alembic**

```bash
cd backend
alembic revision --autogenerate -m "add_device_status"
alembic upgrade head
```
Sprawdź że plik migracji powstał w `migrations/versions/`.

**Step 8: Stwórz `worker/Dockerfile`**

```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY ping_worker.py .

CMD ["python", "ping_worker.py"]
```

**Step 9: Dodaj worker do `deploy/docker-compose.yml`**

Dodaj service po `frontend`:

```yaml
  worker:
    build:
      context: ../worker
      dockerfile: Dockerfile
    environment:
      DATABASE_URL: postgresql+asyncpg://netcore:netcore@postgres:5432/netcore
      INTERVAL: "30"
    depends_on:
      postgres:
        condition: service_healthy
```

**Step 10: Uruchom wszystkie testy — sprawdź że PASS**

```bash
cd backend
pytest ../tests/ -v
```
Oczekiwane: wszystkie passing (nowe testy + stare)

**Step 11: Commit**

```bash
cd source/network-core
git add worker/ deploy/docker-compose.yml backend/app/models/device.py backend/migrations/
git commit -m "feat: ping worker monitors device status, add status column to Device model"
```

---

## Definition of Done — Sprint 2

- [ ] `GET /api/v1/devices/{id}` zwraca 200 lub 404
- [ ] `GET /health/ready` zawiera `{"status":"ok","db":"ok"}`
- [ ] `docker-compose.yml` montuje nmap-data i zawiera frontend + worker
- [ ] Frontend uruchamia się na porcie 5173, wyświetla mapę topologii
- [ ] Worker aktualizuje pole `status` w tabeli devices
- [ ] Wszystkie testy passing (unit + integration)
